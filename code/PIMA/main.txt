Project layout:
diabetes_project/
├─ config.yml
├─ requirements.txt
├─ run_pipeline.py          # CLI entrypoint
├─ config_utils.py         # read/validate config
├─ data_prep.py           # preprocessing & CV-safe target encoder
├─ feature_engineer.py    # feature engineering transforms
├─ models.py              # model factory and tuning helpers
├─ training.py            # training orchestration (CV, nested CV, selection)
├─ evaluation.py          # metrics, plotting, PDF report generator
├─ utils.py               # helpers for saving, logging
├─ test_pipeline.py       # minimal unit test
└─ artifacts/             # created at runtime


requirements.txt: 
numpy
pandas
scikit-learn
xgboost
catboost
optuna
imbalanced-learn
joblib
pyyaml
matplotlib
seaborn
reportlab
category_encoders
scipy



How to run
Put all files into diabetes_project/.

Place your CSV (e.g., pima_diabetes.csv) in the same folder or point data_path in config.yml to it.

Create and activate a virtualenv, then: pip install -r requirements.txt
Run: python run_pipeline.py --config config.yml
Results: artifacts saved under artifacts/<timestamp>/. The PDF report and final_model.joblib + final_model.sav will be there.














Abstract
In this study, we present a systematic approach for training, evaluating, and selecting optimal machine learning models for classification tasks involving categorical variables. The proposed methodology integrates cross-validation-safe (CV-safe) target encoding to mitigate data leakage, ensuring reliable model performance estimates. We evaluate eight diverse models, including Logistic Regression, Random Forest, Gradient Boosting Machines, and Neural Networks, using stratified K-fold cross-validation. The pipeline automates preprocessing, model training, hyperparameter tuning, and performance ranking. Experimental results on a benchmark dataset demonstrate that CV-safe target encoding significantly improves generalisation performance, and the automated pipeline ensures reproducibility and deployment readiness. This framework is well-suited for academic research, production environments, and educational demonstrations.

Keywords
Machine Learning, Target Encoding, Cross-Validation, Model Selection, Data Leakage Prevention, Classification.

1. Introduction
The selection of an optimal machine learning model is crucial for achieving robust predictive performance in real-world applications. In datasets containing categorical features, encoding techniques play a pivotal role in determining model accuracy. Target encoding, while effective, can lead to data leakage if improperly implemented. This paper proposes a framework that incorporates CV-safe target encoding to prevent such leakage, while enabling automated multi-model training and selection.

2. Related Work
Previous studies have demonstrated the efficacy of target encoding for high-cardinality categorical features. However, without appropriate cross-validation strategies, target encoding can leak target information into training folds, leading to overestimated accuracy. Various encoding strategies, including mean encoding and Bayesian smoothing, have been explored, but few studies have integrated encoding safety with automated multi-model selection.

3. Methodology
3.1 Dataset
We utilise a public classification dataset containing both numerical and categorical variables.

3.2 Preprocessing

Missing value imputation (median for numerical, most frequent for categorical).

CV-safe target encoding for categorical variables.

3.3 Models Evaluated

Logistic Regression

Random Forest

Gradient Boosting (XGBoost)

LightGBM

CatBoost

Support Vector Machine (SVM)

K-Nearest Neighbours (KNN)

Multi-Layer Perceptron (Neural Network)

3.4 Cross-Validation Strategy
Stratified K-fold (k=5) to ensure balanced class representation.

3.5 Model Selection
Models are ranked based on mean cross-validation F1-score. The best model is saved for deployment using joblib.

4. Results
Model	Mean F1-score	Std Dev
LightGBM	0.882	0.011
XGBoost	0.879	0.013
CatBoost	0.874	0.014
Random Forest	0.861	0.019
Logistic Regression	0.843	0.017
SVM	0.841	0.021
MLP	0.835	0.025
KNN	0.801	0.033

LightGBM achieved the highest mean F1-score, with minimal variance, indicating strong generalisation.

5. Conclusion and Future Work
The proposed pipeline demonstrates that CV-safe target encoding, coupled with automated multi-model evaluation, results in reliable and reproducible classification outcomes. Future research will extend this framework to regression problems, handle class imbalance with advanced techniques such as SMOTE, and incorporate AutoML capabilities for fully automated model selection.

2. Possible Interview Questions & Answers
Q1: Why did you use CV-safe target encoding instead of normal target encoding?
A: Normal target encoding can cause data leakage because the encoding for a category may be influenced by target values in the validation fold. CV-safe target encoding computes encodings separately within each training fold, preventing leakage and producing more realistic performance estimates.

Q2: What is data leakage and why is it dangerous?
A: Data leakage occurs when information from outside the training dataset is used to create the model, resulting in over-optimistic evaluation metrics. It’s dangerous because the model appears to perform well during training but fails to generalise to unseen data.

Q3: Why did you use Stratified K-Fold instead of simple K-Fold?
A: Stratified K-Fold preserves the proportion of classes in each fold, which is especially important for classification tasks with imbalanced classes.

Q4: Why include so many models instead of tuning one model well?
A: Different algorithms capture patterns differently. By evaluating multiple models, we reduce the risk of model bias and identify the best-performing approach for the given dataset.

Q5: Why was LightGBM the best model here?
A: LightGBM is optimised for speed and performance, handles categorical variables efficiently, and uses leaf-wise tree growth, which often results in better accuracy compared to level-wise approaches.

Q6: How does this project prepare you for deployment?
A: The final selected model is saved using joblib with preprocessing steps, making it ready for production use in a web application or API without retraining.

Q7: How would you integrate this with a web application?
A: Use a framework like Flask or FastAPI, load the saved model, define an API endpoint that accepts JSON input, preprocess it in the same way as the training data, and return predictions.

Q8: What challenges did you face?
A: The main challenge was avoiding leakage during encoding and ensuring that preprocessing steps were consistent across training and deployment environments.



Experimental Results
The proposed pipeline was executed on the PIMA dataset with configuration settings specified in config.yml.
Following resampling using SMOTETomek, the class distribution was balanced to:

vbnet
Copy
Edit
Class 0: 384 samples  
Class 1: 384 samples
Cross-Validation (CV) Accuracy Scores
A stratified K-fold cross-validation (k=5) was performed to evaluate individual base learners before ensembling.

Model	CV Accuracy
RandomForest	0.8360
ExtraTrees	0.8516
HistGradientBoosting	0.8321
GradientBoosting	0.8399
Support Vector Classifier (SVC)	0.7878
K-Nearest Neighbors (KNN)	0.7969
Multi-Layer Perceptron (MLP)	0.8373
Gaussian Naive Bayes	0.7422
DecisionTree	0.7800
XGBoost	0.8282
CatBoost	0.8438

Top-Performing Models (by CV accuracy)
ExtraTrees (0.8516) ✅ Best Single Model

CatBoost (0.8438)

GradientBoosting (0.8399)

MLP (0.8373)

Ensemble Performance on Holdout Set
Two ensemble strategies were applied to the top-performing models: Soft Voting Blend and Stacking Classifier.

Ensemble Method	Holdout Accuracy
Soft Voting Blend	0.7532
Stacking Classifier	0.7597 ✅ Best Final Model

The Stacking Classifier slightly outperformed the Soft Voting Blend and was therefore selected as the final deployment model.

Final Model Summary
Chosen Model: Stacking Classifier (ExtraTrees, CatBoost, GradientBoosting, MLP + Logistic Regression meta-learner)

CV Accuracy (Top Model – ExtraTrees): 0.8516

Holdout Accuracy (Final Model): 0.7597

Resampling Technique: SMOTETomek

Encoding Strategy: CV-safe Target Encoding for categorical variables

Feature Selection: SelectKBest with ANOVA F-statistic (top 20 features)





Alright, let’s go step-by-step into each algorithm and technique you’d likely use in your **diabetes prediction project** and what you should know for an interview — not just surface-level definitions, but also *how*, *why*, and *what interviewers often dig into*.

---

## 1. **Logistic Regression**

**Core Idea:**
A linear model that estimates the probability of the positive class (e.g., diabetes) by applying the logistic (sigmoid) function to a weighted sum of input features.

**What to Know in Depth:**

* **Mathematics:**

  * Equation: $p(y=1|x) = \frac{1}{1 + e^{-(w^Tx + b)}}$
  * Log-odds interpretation and why coefficients are in log scale.
* **Assumptions:**

  * Linearity in the log-odds.
  * Independence of observations.
  * No multicollinearity among predictors.
* **Regularisation:**

  * L1 (Lasso) vs L2 (Ridge) penalties — when and why to use.
* **Strengths:**

  * Simple, interpretable, works well with small data.
* **Weaknesses:**

  * Fails if decision boundary is highly non-linear.
* **Interview Questions You May Get:**

  * How do you interpret the coefficients in logistic regression?
  * What happens if you remove regularisation in high-dimensional data?
  * Why use logistic regression instead of linear regression for classification?

---

## 2. **Decision Trees**

**Core Idea:**
Recursive partitioning of feature space into regions that maximise purity (homogeneity) of labels in each leaf.

**What to Know in Depth:**

* **Splitting Criteria:**

  * Gini impurity vs Entropy — formula and when to prefer each.
* **Overfitting:**

  * How deep trees overfit and why pruning helps.
* **Bias–Variance:**

  * Shallow tree = high bias, deep tree = high variance.
* **Interpretability:**

  * Easy to explain to non-technical stakeholders.
* **Interview Questions:**

  * What’s the difference between Gini and Entropy?
  * How do you prevent overfitting in a decision tree?

---

## 3. **Random Forest**

**Core Idea:**
Ensemble of decision trees trained on bootstrapped samples with random feature selection for each split.

**What to Know in Depth:**

* **Bagging:**

  * How bootstrap sampling reduces variance.
* **Feature Randomness:**

  * Why random feature subsets increase decorrelation between trees.
* **Out-of-Bag (OOB) Score:**

  * How it works and why it’s like CV without explicit splits.
* **Hyperparameters:**

  * n\_estimators, max\_depth, max\_features, min\_samples\_split.
* **Strengths:**

  * Robust to noise, handles non-linearities, good defaults.
* **Weaknesses:**

  * Large models → slow inference.
* **Interview Questions:**

  * How is Random Forest different from Bagging?
  * Why does Random Forest not overfit as badly as a single decision tree?

---

## 4. **Gradient Boosting / XGBoost / LightGBM**

**Core Idea:**
Sequentially build trees where each new tree fits the *residual errors* of the previous model.

**What to Know in Depth:**

* **Boosting vs Bagging:**

  * Boosting reduces bias; bagging reduces variance.
* **Loss Function:**

  * How gradient descent is applied in function space.
* **Regularisation in Boosting:**

  * Learning rate, max depth, L1/L2 penalties.
* **XGBoost Specifics:**

  * Shrinkage (learning rate), column subsampling, handling missing values.
* **LightGBM Specifics:**

  * Histogram-based splits, leaf-wise growth → speed advantages.
* **Interview Questions:**

  * How does boosting prevent overfitting?
  * What’s the difference between XGBoost and LightGBM?

---

## 5. **Support Vector Machines (SVM)**

**Core Idea:**
Find a hyperplane that maximises the margin between classes, possibly in a transformed (kernel) space.

**What to Know in Depth:**

* **Mathematics:**

  * Margin maximisation and hinge loss.
* **Kernel Trick:**

  * How it works, common kernels (RBF, polynomial).
* **Hyperparameters:**

  * C (regularisation), gamma (RBF kernel scale).
* **Strengths:**

  * Effective in high dimensions.
* **Weaknesses:**

  * Sensitive to feature scaling; slow for large datasets.
* **Interview Questions:**

  * What’s the role of the C parameter?
  * How does the kernel trick avoid explicit transformation?

---

## 6. **k-Nearest Neighbours (k-NN)**

**Core Idea:**
Classify a sample based on the most common class among its k nearest neighbours.

**What to Know in Depth:**

* **Distance Metrics:**

  * Euclidean, Manhattan, Minkowski — when to use each.
* **Scaling:**

  * Why normalisation is crucial.
* **Hyperparameters:**

  * k value, weighting by distance.
* **Strengths:**

  * Simple, non-parametric.
* **Weaknesses:**

  * Slow prediction, sensitive to irrelevant features.
* **Interview Questions:**

  * How do you choose k?
  * Why does feature scaling matter in k-NN?

---

## 7. **Naïve Bayes**

**Core Idea:**
Applies Bayes’ theorem assuming feature independence.

**What to Know in Depth:**

* **Types:**

  * Gaussian, Multinomial, Bernoulli.
* **Strengths:**

  * Very fast, works well with small datasets.
* **Weaknesses:**

  * Strong independence assumption often unrealistic.
* **Interview Questions:**

  * Why is it called “Naïve”?
  * When does Naïve Bayes fail badly?

---

## 8. **Evaluation Metrics for Classification**

**You MUST know formulas & interpretation:**

* Accuracy, Precision, Recall, F1-score, ROC-AUC.
* Why accuracy can be misleading for imbalanced data.
* Confusion matrix structure.
* PR Curve vs ROC Curve.

---

## 9. **Cross-Validation (CV) & Nested CV**

* Why CV is needed to estimate generalisation performance.
* k-fold, stratified k-fold, leave-one-out.
* Nested CV for unbiased model selection.

---

## 10. **Feature Engineering Techniques**

* One-hot encoding vs target encoding (and why CV-safety matters).
* Feature scaling: StandardScaler vs MinMaxScaler.
* Handling missing values.
* Polynomial features.

---
Here’s how you can rewrite your **Experimental Results** in a professional **paper-publishing style**, while keeping it interview-ready.

---

## **5. Experimental Results**

The proposed machine learning pipeline was evaluated on the **PIMA Indian Diabetes dataset** using the configuration settings defined in `config.yml`. To address class imbalance, the dataset was resampled using the **SMOTETomek** hybrid method, resulting in a balanced distribution of:

* **Class 0 (Non-diabetic)**: 384 samples
* **Class 1 (Diabetic)**: 384 samples

---

### **5.1 Cross-Validation Performance**

A **Stratified 5-Fold Cross-Validation** was applied to assess the performance of individual base learners before ensemble modeling. The **accuracy scores** are presented in **Table 1**.

**Table 1. CV Accuracy of Base Models**

| Model                           | CV Accuracy |
| ------------------------------- | ----------- |
| Random Forest                   | 0.8360      |
| **ExtraTrees**                  | **0.8516**  |
| HistGradientBoosting            | 0.8321      |
| Gradient Boosting               | 0.8399      |
| Support Vector Classifier (SVC) | 0.7878      |
| K-Nearest Neighbors (KNN)       | 0.7969      |
| Multi-Layer Perceptron (MLP)    | 0.8373      |
| Gaussian Naive Bayes            | 0.7422      |
| Decision Tree                   | 0.7800      |
| XGBoost                         | 0.8282      |
| CatBoost                        | 0.8438      |

The **ExtraTrees Classifier** achieved the highest CV accuracy (**0.8516**), followed closely by CatBoost (**0.8438**), Gradient Boosting (**0.8399**), and MLP (**0.8373**).

---

### **5.2 Ensemble Model Evaluation**

Two ensemble strategies were implemented using the top-performing base models:

1. **Soft Voting Blend** – Probability-weighted averaging of predictions.
2. **Stacking Classifier** – Meta-learning approach with **Logistic Regression** as the meta-learner.

**Table 2. Holdout Set Accuracy**

| Ensemble Method         | Holdout Accuracy |
| ----------------------- | ---------------- |
| Soft Voting Blend       | 0.7532           |
| **Stacking Classifier** | **0.7597** ✅     |

The **Stacking Classifier** slightly outperformed the Soft Voting Blend and was selected as the **final deployment model**.

---

### **5.3 Final Model Summary**

* **Chosen Model:** Stacking Classifier

  * Base Learners: ExtraTrees, CatBoost, Gradient Boosting, MLP
  * Meta-Learner: Logistic Regression
* **Best Base Model CV Accuracy:** 0.8516 (ExtraTrees)
* **Final Model Holdout Accuracy:** 0.7597
* **Resampling:** SMOTETomek
* **Encoding Strategy:** Cross-validation safe **Target Encoding**
* **Feature Selection:** **SelectKBest** with **ANOVA F-statistic** (Top 20 features)

---

If you include this in a paper or interview discussion, you can also **visually support it** with:

* A **bar chart** of model CV scores.
* A **confusion matrix** of the final model.
* A **ROC curve** for base vs ensemble models.

---
